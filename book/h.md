## Hadoop
Apache Hadoop ( /həˈduːp/) is a framework to handle big data storage and process amoung one to thousands computers. It's mature and initiated in early stage of industry, a lot of other big data framework can be integrated to it. You can DIY your bigdata solution with hadoop and other modules. 

###
Core modules
 - Yarn: the coordinator for job scheduling and resource management
 - HDFS: distributed storage system
 - MapReduce: distributed compute system for parallel data processing
 - Common: support other modules


## HPC
High-performance computing focuses on build high performance computer clusters for fast speed calculations, like AI, science research and storm predict. 
